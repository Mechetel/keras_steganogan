{"cells":[{"cell_type":"markdown","metadata":{"id":"qYnJJYUxbdua"},"source":["# SteganoGAN in Keras\n","This notebook contains code attempting to reimplement SteganoGAN in Keras, for the purpose of better understanding (and scrutinizing) it.\n","\n","*Based on https://github.com/DAI-Lab/SteganoGAN/tree/master/steganogan*"]},{"cell_type":"markdown","metadata":{"id":"OTRQl5_KUxUA"},"source":["### Modules"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"QbnEM8Oubduh"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-22 23:27:05.017521: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["import os\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","\n","import sys\n","sys.path.append(\"../..\")\n","sys.path.append(\"../../..\")\n","\n","import tensorflow as tf\n","from keras.optimizers import Adam\n","from keras.losses import BinaryCrossentropy\n","from tensorflow.keras.callbacks import CSVLogger\n","from callbacks import Checkpoint, SaveImages\n","\n","from models import (\n","  steganogan_encoder_residual_model,\n","  steganogan_decoder_basic_model,\n","  steganogan_critic_model\n",")\n","\n","from dataset_utils import normalize_img, create_message_dataset\n","from keras_steganogan import KerasSteganoGAN"]},{"cell_type":"markdown","metadata":{},"source":["### Constants"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Image dimensions\n","IMAGE_HEIGHT = 128\n","IMAGE_WIDTH = 128\n","IMAGE_CHANNELS = 3\n","\n","IMAGE_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)\n","MESSAGE_DEPTH = 4\n","BATCH_SIZE = 4\n","MODEL_PATH = f'../../pretrained_models/{MESSAGE_DEPTH}/steganoGAN_residual.keras'\n","LOGS_PATH = f'../../logs/residual/{MESSAGE_DEPTH}/steganoGAN_residual.csv'\n","CALLBACK_IMAGES_PATH = '../../../images/callback'\n","CALLBACK_IMAGES_OUTPUT_PATH = f'../../epoch_images/residual/{MESSAGE_DEPTH}'"]},{"cell_type":"markdown","metadata":{},"source":["----"]},{"cell_type":"markdown","metadata":{},"source":["### Build model for future train"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["encoder = steganogan_encoder_residual_model(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS, MESSAGE_DEPTH)\n","decoder = steganogan_decoder_basic_model(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS, MESSAGE_DEPTH)\n","critic = steganogan_critic_model(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)\n","\n","steganoGAN = KerasSteganoGAN(\n","  encoder=encoder,\n","  decoder=decoder,\n","  critic=critic,\n","  image_shape=IMAGE_SHAPE,\n","  data_depth=MESSAGE_DEPTH,\n","  model_path=MODEL_PATH\n",")\n","\n","steganoGAN.compile(\n","  encoder_optimizer = Adam(learning_rate=1e-4, beta_1=0.5),\n","  decoder_optimizer = Adam(learning_rate=1e-4, beta_1=0.5),\n","  critic_optimizer = Adam(learning_rate=1e-4, beta_1=0.5),\n","  loss_fn = BinaryCrossentropy(from_logits=False)\n",")\n","\n","#steganoGAN.models_summary()\n","#steganoGAN.summary()"]},{"cell_type":"markdown","metadata":{},"source":["### Download div2k dataset and complete it with random message dataset of {0, 1}"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 800 files.\n","Found 100 files.\n"]}],"source":["train_dir = '/Users/dmitryhoma/Projects/phd_dissertation/state_2/SteganoGAN/research/data/div2k/train'\n","val_dir = '/Users/dmitryhoma/Projects/phd_dissertation/state_2/SteganoGAN/research/data/div2k/val'\n","\n","train_image_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    train_dir, \n","    label_mode=None, \n","    color_mode='rgb',\n","    batch_size=BATCH_SIZE,\n","    seed=123,\n","    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n","    shuffle=True\n",")\n","\n","val_image_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    val_dir, \n","    label_mode=None, \n","    color_mode='rgb',\n","    batch_size=BATCH_SIZE,\n","    seed=123,\n","    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n","    shuffle=True\n",")\n","\n","train_image_ds = train_image_ds.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n","val_image_ds = val_image_ds.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","train_message_ds = create_message_dataset(BATCH_SIZE, len(train_image_ds), IMAGE_HEIGHT, IMAGE_WIDTH, MESSAGE_DEPTH)\n","val_message_ds = create_message_dataset(BATCH_SIZE, len(val_image_ds), IMAGE_HEIGHT, IMAGE_WIDTH, MESSAGE_DEPTH)\n","\n","train_ds = tf.data.Dataset.zip((train_image_ds, train_message_ds)).prefetch(buffer_size=tf.data.AUTOTUNE)\n","val_ds = tf.data.Dataset.zip((val_image_ds, val_message_ds)).prefetch(buffer_size=tf.data.AUTOTUNE)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 973ms/step - critic_loss: -4.8634e-04 - decoder_accuracy: 0.5232 - decoder_loss: 0.7633 - encoder_decoder_total_loss: 0.9613 - psnr: 7.8134 - realism_loss: 6.3118e-04 - rs_bpp: 0.1860 - similarity_loss: 0.1974 - ssim: 0.3217 - val_critic_loss: -0.0028 - val_decoder_accuracy: 0.5743 - val_decoder_loss: 0.6794 - val_encoder_decoder_total_loss: 0.7105 - val_psnr: 15.1980 - val_realism_loss: 5.9018e-04 - val_rs_bpp: 0.5947 - val_similarity_loss: 0.0305 - val_ssim: 0.5804\n","Epoch 2/5\n","\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 966ms/step - critic_loss: -0.0055 - decoder_accuracy: 0.6325 - decoder_loss: 0.6450 - encoder_decoder_total_loss: 0.6846 - psnr: 14.2611 - realism_loss: 7.5863e-04 - rs_bpp: 1.0601 - similarity_loss: 0.0389 - ssim: 0.5568 - val_critic_loss: -0.0103 - val_decoder_accuracy: 0.7029 - val_decoder_loss: 0.5917 - val_encoder_decoder_total_loss: 0.6192 - val_psnr: 14.4963 - val_realism_loss: -0.0087 - val_rs_bpp: 1.6231 - val_similarity_loss: 0.0361 - val_ssim: 0.5538\n","Epoch 3/5\n","\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 963ms/step - critic_loss: -0.0120 - decoder_accuracy: 0.7333 - decoder_loss: 0.5366 - encoder_decoder_total_loss: 0.5577 - psnr: 14.4399 - realism_loss: -0.0157 - rs_bpp: 1.8666 - similarity_loss: 0.0369 - ssim: 0.5272 - val_critic_loss: -0.0243 - val_decoder_accuracy: 0.7857 - val_decoder_loss: 0.4658 - val_encoder_decoder_total_loss: 0.4862 - val_psnr: 13.4414 - val_realism_loss: -0.0262 - val_rs_bpp: 2.2856 - val_similarity_loss: 0.0466 - val_ssim: 0.4902\n","Epoch 4/5\n","\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 989ms/step - critic_loss: -0.0283 - decoder_accuracy: 0.8012 - decoder_loss: 0.4332 - encoder_decoder_total_loss: 0.4461 - psnr: 13.8704 - realism_loss: -0.0294 - rs_bpp: 2.4092 - similarity_loss: 0.0423 - ssim: 0.4815 - val_critic_loss: -0.0422 - val_decoder_accuracy: 0.8348 - val_decoder_loss: 0.3753 - val_encoder_decoder_total_loss: 0.3997 - val_psnr: 12.8689 - val_realism_loss: -0.0295 - val_rs_bpp: 2.6781 - val_similarity_loss: 0.0539 - val_ssim: 0.4370\n","Epoch 5/5\n","\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 998ms/step - critic_loss: -0.0451 - decoder_accuracy: 0.8410 - decoder_loss: 0.3605 - encoder_decoder_total_loss: 0.3786 - psnr: 13.2134 - realism_loss: -0.0316 - rs_bpp: 2.7283 - similarity_loss: 0.0498 - ssim: 0.4318 - val_critic_loss: -0.0542 - val_decoder_accuracy: 0.8624 - val_decoder_loss: 0.3208 - val_encoder_decoder_total_loss: 0.3462 - val_psnr: 12.7756 - val_realism_loss: -0.0304 - val_rs_bpp: 2.8994 - val_similarity_loss: 0.0558 - val_ssim: 0.4065\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x15491fe00>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["steganoGAN.build([(1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS), (1, IMAGE_HEIGHT, IMAGE_WIDTH, MESSAGE_DEPTH)])\n","steganoGAN.fit(train_ds, epochs=5, validation_data=val_ds, callbacks=[\n","  SaveImages(MESSAGE_DEPTH, IMAGE_SHAPE, CALLBACK_IMAGES_PATH, CALLBACK_IMAGES_OUTPUT_PATH),\n","  Checkpoint(MODEL_PATH),\n","  CSVLogger(LOGS_PATH)\n","])"]}],"metadata":{"colab":{"name":"steganogan_keras.ipynb","provenance":[{"file_id":"https://github.com/jnickg/steganet/blob/main/steganogan_keras.ipynb","timestamp":1710610773710}]},"kernelspec":{"display_name":"tf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}
