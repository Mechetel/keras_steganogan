{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jnickg/steganet/blob/main/steganogan_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYnJJYUxbdua"
      },
      "source": [
        "# SteganoGAN in Keras\n",
        "This notebook contains code attempting to reimplement SteganoGAN in Keras, for the purpose of better understanding (and scrutinizing) it.\n",
        "\n",
        "*Based on https://github.com/DAI-Lab/SteganoGAN/tree/master/steganogan*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start with the basic `Conv2D` layer, which is used in vairous parts of the overall model. Batch normalization comes after the activation, as seen [here](https://github.com/DAI-Lab/SteganoGAN/blob/master/steganogan/encoders.py#L117-L121)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sub-networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "QbnEM8Oubduh"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "def steganogan_conv2d_layer(layer_in, num_filters, kernel_size, name=None, normalize=True, activation_fn=LeakyReLU()):\n",
        "    model = Conv2D(num_filters, kernel_size, padding='same', activation=activation_fn, name=name)(layer_in)\n",
        "    if normalize:\n",
        "        normalize_name = f'{name}_normalize' if name is not None else None\n",
        "        model = BatchNormalization(name=normalize_name)(model)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder\n",
        "We will focus on the Dense variant of SteganoGAN, since researchers reported getting the best results with hit. This takes four hyperparameters: The image dimensions $(W, H, C)$, and the depth of the data to be encoded $D$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "H2mhSvCYbdui"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.layers import Add\n",
        "from tensorflow.keras.activations import sigmoid\n",
        "\n",
        "def steganogan_encoder_dense_model(W, H, C, D):\n",
        "    \"\"\"\n",
        "    The BasicEncoder module takes an cover image and a data tensor and combines\n",
        "    them into a steganographic image.\n",
        "    Input: (N, 3, H, W), (N, D, H, W)\n",
        "    Output: (N, 3, H, W)\n",
        "    \"\"\"\n",
        "    input_image = Input(shape=(W, H, C), name=f'image{W}x{H}x{C}')\n",
        "    input_data  = Input(shape=(W, H, D), name=f'data{W}x{H}x{D}')\n",
        "\n",
        "    image_preprocess = steganogan_conv2d_layer(input_image, 32, 3, name='image_preprocess')\n",
        "\n",
        "    image_data_process_1_in = Concatenate(name='image_data_process_1_in')([image_preprocess, input_data])\n",
        "    image_data_process_1 = steganogan_conv2d_layer(image_data_process_1_in, 32, 3, name='image_data_process_1')\n",
        "\n",
        "    image_data_process_2_in = Concatenate(name='image_data_process_2_in')([image_preprocess, image_data_process_1, input_data])\n",
        "    image_data_process_2 = steganogan_conv2d_layer(image_data_process_2_in, 32, 3, name='image_data_process_2')\n",
        "\n",
        "    encoder_in = Concatenate(name='encoder_in')([image_preprocess, image_data_process_1, image_data_process_2, input_data])\n",
        "    encoder = steganogan_conv2d_layer(encoder_in, 3, 3, name='encoder', normalize=False, activation_fn=sigmoid)\n",
        "\n",
        "    encoder_out = Add(name='encoder_out')([input_image, encoder])\n",
        "\n",
        "    return ([input_image, input_data], encoder_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decoder\n",
        "\n",
        "Similarly, the decoder takes the same hyperparameters. We simplify usage a bit by reshaping the output to be a 1D vector of the decoded data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "kkUkzdhUbduj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Reshape\n",
        "\n",
        "def steganogan_decoder_dense_model(W, H, C, D):\n",
        "    \"\"\"\n",
        "    The DenseDecoder module takes an steganographic image and attempts to decode\n",
        "    the embedded data tensor.\n",
        "    Input: (N, 3, H, W)\n",
        "    Output: (N, D, H, W)\n",
        "    \"\"\"\n",
        "    input = Input(shape=(W, H, C), name=f'cover_image{W}x{H}x{C}')\n",
        "    decode_1 = steganogan_conv2d_layer(input, 32, 3, name='decode_1')\n",
        "\n",
        "    decode_2 = steganogan_conv2d_layer(decode_1, 32, 3, name='decode_2')\n",
        "\n",
        "    decode_3_in = Concatenate(name='decode_3_in')([decode_1, decode_2])\n",
        "    decode_3 = steganogan_conv2d_layer(decode_3_in, 32, 3, name='decode_3')\n",
        "\n",
        "    decoder_in = Concatenate(name='decoder_in')([decode_1, decode_2, decode_3])\n",
        "    decoder = steganogan_conv2d_layer(decoder_in, D, 3, name='decoder', normalize=False, activation_fn=sigmoid)\n",
        "    decoder = Reshape((1,-1))(decoder)\n",
        "\n",
        "    return input, decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Critic\n",
        "\n",
        "The critic also takes the same hyperparameters, and produces a single-element tensor as its output: the average of the final convolutional layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "1r2ozQ7mbduj"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "\n",
        "def steganogan_critic_model(W, H, C, D):\n",
        "    \"\"\"\n",
        "    The BasicCritic module takes an image and predicts whether it is a cover\n",
        "    image or a steganographic image (N, 1).\n",
        "    Input: (N, 3, H, W)\n",
        "    Output: (N, 1)\n",
        "    \"\"\"\n",
        "    model_in = Input(shape=(W, H, C), name=f'image{W}x{H}x{C}')\n",
        "    model = steganogan_conv2d_layer(model_in, 32, 3, name='conv_1')\n",
        "    model = steganogan_conv2d_layer(model, 32, 3, name='conv_2')\n",
        "    model = steganogan_conv2d_layer(model, 32, 3, name='conv_3')\n",
        "    model = steganogan_conv2d_layer(model, 1, 3, name='conv_4', normalize=False, activation_fn=sigmoid)\n",
        "    model = AveragePooling2D(pool_size=(model.shape[1], model.shape[2]), name='mean')(model)\n",
        "    return model_in, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overall Model\n",
        "\n",
        "We implement the overall model as a subclass of `keras.Model`, which makes it easier to train. It is initialized with the inputs and outputs for each subnetwork, plus the hyperparameters used to create those. Finally, there is a `noise_func`, which is a data generator function. This allows the model to generate user-specified random data, for encoding/decoding steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "class KerasSteganoGAN(tf.keras.Model):\n",
        "  def __init__(self, gen_in, gen_out, dec_in, dec_out, critic_in, critic_out, image_height, image_width, image_channels, data_depth, noise_func=None):\n",
        "    super(KerasSteganoGAN, self).__init__()\n",
        "\n",
        "    # Metadata regarding discrminator input images\n",
        "    self.img_rows = image_height\n",
        "    self.img_cols = image_width\n",
        "    self.channels = image_channels\n",
        "    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "    \n",
        "    # Metadata regarding generator inputdata\n",
        "    self.data_depth = data_depth\n",
        "    self.noise_func = noise_func\n",
        "\n",
        "    # Build and compile the critic\n",
        "    self.critic = Model(inputs=critic_in, outputs=critic_out, name='KerasSteganoGAN_critic')\n",
        "\n",
        "    # Build the generator\n",
        "    self.encoder = Model(inputs=gen_in, outputs=gen_out, name='KerasSteganoGAN_encoder')\n",
        "    self.decoder = Model(inputs=dec_in, outputs=dec_out, name='KerasSteganoGAN_decoder')\n",
        "    \n",
        "  def compile(self,\n",
        "              d_optimizer=Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "              g_optimizer=Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "              loss=BinaryCrossentropy(),\n",
        "              disc_noise_in=(0.0, 4e-2),\n",
        "              metrics=['loss']):\n",
        "    super(KerasSteganoGAN, self).compile()\n",
        "    self.loss_fn = loss\n",
        "    self.d_optimizer = d_optimizer\n",
        "    self.g_optimizer = g_optimizer\n",
        "    self.disc_noise_in=disc_noise_in\n",
        "\n",
        "  def generator_noise(self, batch_size=1):\n",
        "    return noise_func(batch_size, data_depth)\n",
        "\n",
        "  def critic_loss(self, real_output, fake_output):\n",
        "    real_labels = tf.zeros_like(real_output)\n",
        "    fake_labels = tf.ones_like(fake_output)\n",
        "    # Calculate loss comparing real outputs (\"not fake\") to zeros, and fake\n",
        "    # outputs (\"yes fake\") to ones.\n",
        "    real_loss = self.loss_fn(real_labels, real_output)\n",
        "    fake_loss = self.loss_fn(fake_labels, fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "  def generator_loss(self, fake_output):\n",
        "    # Calculate loss assuming that they should have been evaluated as real, i.e.\n",
        "    # zero, or \"not fake\"\n",
        "    return self.loss_fn(tf.zeros_like(fake_output), fake_output)\n",
        "\n",
        "  def call(self, input_tensor, training=False):\n",
        "    x = input_tensor\n",
        "    x = self.critic(x, training=training)\n",
        "    return x\n",
        "\n",
        "  def train_step(self, real_images):\n",
        "    # If we were give an (x,y) tuple, drop the labels because we just don't care\n",
        "    if isinstance(real_images, tuple):\n",
        "      real_images = real_images[0]\n",
        "\n",
        "    # We need to know how big our batches are, so we can create sets of\n",
        "    # generated images in the same quantity\n",
        "    batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "    # Train critic first, with real images AND a batch of fakes. Its loss\n",
        "    # is determined by how well it can discern between real and fake images\n",
        "    seeds = self.encoder_noise(batch_size=batch_size)\n",
        "\n",
        "    #gen_inputs = \n",
        "    generated_images = self.encoder(seeds, training=True)\n",
        "\n",
        "    # If specified, apply noise to prevent critic from cheating\n",
        "    if (self.disc_noise_in is not None):\n",
        "      (noise_mean, noise_sd) = self.disc_noise_in\n",
        "      noise_shape = [batch_size, self.img_rows, self.img_cols, self.channels]\n",
        "      real_images = real_images + tf.random.normal(noise_shape, mean=noise_mean, stddev=noise_sd)\n",
        "      generated_images = generated_images + tf.random.normal(noise_shape, mean=noise_mean, stddev=noise_sd)\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      real_predictions = self.critic(real_images, training=True)\n",
        "      fake_predictions = self.critic(generated_images, training=True)\n",
        "\n",
        "      disc_loss = self.critic_loss(real_predictions, fake_predictions)\n",
        "\n",
        "    disc_grad = disc_tape.gradient(disc_loss,\n",
        "                                   self.critic.trainable_weights)\n",
        "    self.d_optimizer.apply_gradients(zip(disc_grad,\n",
        "                                         self.critic.trainable_weights))\n",
        "\n",
        "    # Train generator by pitting it against the updated critic. Its loss\n",
        "    # is determined by how well it can trick the critic.\n",
        "    gen_seeds = self.encoder_noise(batch_size=batch_size)\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      new_generated_images = self.encoder(gen_seeds, training=True)\n",
        "      predictions_on_gen = self.critic(new_generated_images, training=True)\n",
        "      gen_loss = self.encoder_loss(predictions_on_gen)\n",
        "      \n",
        "    gen_grad = gen_tape.gradient(gen_loss,\n",
        "                                 self.encoder.trainable_weights)\n",
        "    self.g_optimizer.apply_gradients(zip(gen_grad,\n",
        "                                         self.encoder.trainable_weights))\n",
        "    \n",
        "\n",
        "    return {'gen_loss': gen_loss, 'disc_loss': disc_loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters\n",
        "We list out the hyperparameters here, so they are consolidated into a single space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image dimensions\n",
        "my_W = 128\n",
        "my_H = 128\n",
        "my_C = 3\n",
        "# Data dimension (along with my_W and my_H)\n",
        "my_D = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Generator\n",
        "\n",
        "Let's define a simple function we can use to generate data for the encoder to put into carrier images. We'll start by using a lorem ipsum generator, and later use it with a custom dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's a sample usage of the API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lorem ipsum.\n"
          ]
        }
      ],
      "source": [
        "from loremipsum import generate_sentences\n",
        "\n",
        "sentences = generate_sentences(1, start_with_lorem=True)\n",
        "for s in sentences:\n",
        "  print(s[2])\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then, we bake it into a function with a simple adapter interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_data(quantity, depth):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we put it all together and create our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MF18XAt_bdui",
        "outputId": "b11d2d8c-a499-49ed-eb60-e8fab1972733"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_steganogan(subnet_summary=False):\n",
        "  encoder_in, encoder_out = steganogan_encoder_dense_model(my_W, my_H, my_C, my_D)\n",
        "  steganogan_encoder = Model(encoder_in, encoder_out, name='steganogan_encoder')\n",
        "  if (subnet_summary):\n",
        "    steganogan_encoder.summary()\n",
        "\n",
        "  decoder_in, decoder_out = steganogan_decoder_dense_model(my_W, my_H, my_C, my_D)\n",
        "  steganogan_decoder = Model(decoder_in, decoder_out, name='steganogan_decoder')\n",
        "  if (subnet_summary):\n",
        "    steganogan_decoder.summary()\n",
        "\n",
        "  critic_in, critic_out = steganogan_critic_model(my_W, my_H, my_C, my_D)\n",
        "  steganogan_critic = Model(critic_in, critic_out, name='steganogan_critic')\n",
        "  if (subnet_summary):\n",
        "    steganogan_critic.summary()\n",
        "\n",
        "  steganoGAN = KerasSteganoGAN(encoder_in, encoder_out, decoder_in, decoder_out, critic_in, critic_out, my_H, my_W, my_C, my_D)\n",
        "  steganoGAN.build((None, my_H, my_W, my_C))\n",
        "  steganoGAN.summary()\n",
        "  return steganoGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Actually creating it prints out the parameters for each sub-network, and then prints out a final summary of the consolidated model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"steganogan_encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image128x128x3 (InputLayer)     [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "image_preprocess (Conv2D)       (None, 128, 128, 32) 896         image128x128x3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "image_preprocess_normalize (Bat (None, 128, 128, 32) 128         image_preprocess[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "data128x128x2 (InputLayer)      [(None, 128, 128, 2) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_1_in (Concat (None, 128, 128, 34) 0           image_preprocess_normalize[0][0] \n",
            "                                                                 data128x128x2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_1 (Conv2D)   (None, 128, 128, 32) 9824        image_data_process_1_in[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_1_normalize  (None, 128, 128, 32) 128         image_data_process_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_2_in (Concat (None, 128, 128, 66) 0           image_preprocess_normalize[0][0] \n",
            "                                                                 image_data_process_1_normalize[0]\n",
            "                                                                 data128x128x2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_2 (Conv2D)   (None, 128, 128, 32) 19040       image_data_process_2_in[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "image_data_process_2_normalize  (None, 128, 128, 32) 128         image_data_process_2[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "encoder_in (Concatenate)        (None, 128, 128, 98) 0           image_preprocess_normalize[0][0] \n",
            "                                                                 image_data_process_1_normalize[0]\n",
            "                                                                 image_data_process_2_normalize[0]\n",
            "                                                                 data128x128x2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "encoder (Conv2D)                (None, 128, 128, 3)  2649        encoder_in[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "encoder_out (Add)               (None, 128, 128, 3)  0           image128x128x3[0][0]             \n",
            "                                                                 encoder[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 32,793\n",
            "Trainable params: 32,601\n",
            "Non-trainable params: 192\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"steganogan_decoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "cover_image128x128x3 (InputLaye [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decode_1 (Conv2D)               (None, 128, 128, 32) 896         cover_image128x128x3[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "decode_1_normalize (BatchNormal (None, 128, 128, 32) 128         decode_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "decode_2 (Conv2D)               (None, 128, 128, 32) 9248        decode_1_normalize[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decode_2_normalize (BatchNormal (None, 128, 128, 32) 128         decode_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "decode_3_in (Concatenate)       (None, 128, 128, 64) 0           decode_1_normalize[0][0]         \n",
            "                                                                 decode_2_normalize[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decode_3 (Conv2D)               (None, 128, 128, 32) 18464       decode_3_in[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "decode_3_normalize (BatchNormal (None, 128, 128, 32) 128         decode_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "decoder_in (Concatenate)        (None, 128, 128, 96) 0           decode_1_normalize[0][0]         \n",
            "                                                                 decode_2_normalize[0][0]         \n",
            "                                                                 decode_3_normalize[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder (Conv2D)                (None, 128, 128, 2)  1730        decoder_in[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 1, 32768)     0           decoder[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 30,722\n",
            "Trainable params: 30,530\n",
            "Non-trainable params: 192\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"steganogan_critic\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image128x128x3 (InputLayer)  [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv_1 (Conv2D)              (None, 128, 128, 32)      896       \n",
            "_________________________________________________________________\n",
            "conv_1_normalize (BatchNorma (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_2 (Conv2D)              (None, 128, 128, 32)      9248      \n",
            "_________________________________________________________________\n",
            "conv_2_normalize (BatchNorma (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_3 (Conv2D)              (None, 128, 128, 32)      9248      \n",
            "_________________________________________________________________\n",
            "conv_3_normalize (BatchNorma (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "conv_4 (Conv2D)              (None, 128, 128, 1)       289       \n",
            "_________________________________________________________________\n",
            "mean (AveragePooling2D)      (None, 1, 1, 1)           0         \n",
            "=================================================================\n",
            "Total params: 20,065\n",
            "Trainable params: 19,873\n",
            "Non-trainable params: 192\n",
            "_________________________________________________________________\n",
            "Model: \"keras_stegano_gan_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "KerasSteganoGAN_critic (Func (None, 1, 1, 1)           20065     \n",
            "_________________________________________________________________\n",
            "KerasSteganoGAN_encoder (Fun (None, 128, 128, 3)       32793     \n",
            "_________________________________________________________________\n",
            "KerasSteganoGAN_decoder (Fun (None, 1, 32768)          30722     \n",
            "=================================================================\n",
            "Total params: 83,580\n",
            "Trainable params: 83,004\n",
            "Non-trainable params: 576\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "steganoGAN = build_steganogan(subnet_summary=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function displays a batch of images arranged into a grid, for instrumenting the training of our model. We'll be able to see a few test images along the way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from math import sqrt\n",
        "\n",
        "def display_batch(images, count, vmin=None, vmax=None):\n",
        "  w = int(sqrt(count))\n",
        "  f = plt.figure(figsize=(w, w))\n",
        "  f.set_size_inches(w+2, w+2)\n",
        "  gs1 = gridspec.GridSpec(w, w)\n",
        "  gs1.update(wspace=0.025, hspace=0.05)\n",
        "  for i in range(count):\n",
        "    current_image = np.array(images[i, :, :, 0])\n",
        "    # define subplot\n",
        "    plt.subplot(w, w, 1 + i)\n",
        "    # turn off axis\n",
        "    plt.axis('off')\n",
        "    # plot raw pixel data\n",
        "    plt.imshow(current_image, cmap='gray', vmin=vmin, vmax=vmax)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The class below was created for a normal GAN, but serves as a callback to display batches of test images (using the above function), throughout training.\n",
        "\n",
        "It will need some modification to support SteganoGAN, whose architecture does not totally align with a standard GAN (i.e. not just \"latent noise\" as the generator input; an actual image is needed too)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GanSampler(keras.callbacks.Callback):\n",
        "  def __init__(self, print_seed=False, sample_square_width=1, epoch_interval=10):\n",
        "    super(GanSampler, self).__init__()\n",
        "    self.print_seed = print_seed\n",
        "    self.num_samples = sample_square_width * sample_square_width\n",
        "    self.w = sample_square_width\n",
        "    self.epoch_interval = epoch_interval\n",
        "\n",
        "  def display_fixed_seeds(self):\n",
        "    generated_images = self.model.generator(self.fixed_seed)\n",
        "    display_batch(generated_images, self.num_samples)\n",
        "\n",
        "  def on_train_begin(self, logs=None):\n",
        "    self.fixed_seed = self.model.generator_noise(batch_size=self.num_samples)\n",
        "\n",
        "    if self.print_seed:\n",
        "      print(f'\\nFixed seeds for sampling: {self.fixed_seed}')\n",
        "    \n",
        "    print('\\nGenerated images using fixed seeds, before ANY training:')\n",
        "    self.display_fixed_seeds()\n",
        "\n",
        "  def on_train_end(self, logs=None):\n",
        "    print('\\nFinal generated images using fixed seeds')\n",
        "    self.display_fixed_seeds()\n",
        "  \n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    if (epoch % self.epoch_interval == 0):\n",
        "      print(f'\\nGenerated image at end of epoch {epoch+1}, using fixed seeds')\n",
        "      self.display_fixed_seeds()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "steganogan_keras.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.1 64-bit (conda)",
      "name": "python391jvsc74a57bd07d704ea16be1f842a0e496abdad6f3afdbae930fc438ffef0a0d5019c690fcb5"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
